{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinbba77/handson-llm/blob/main/HuggingFace_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUrO3VKW3uY"
      },
      "source": [
        "# Hugging Face Transformers\n",
        "\n",
        "## ğŸ”¹ 1. Hugging Faceë€?\n",
        "- Hugging FaceëŠ” **AI ëª¨ë¸(íŠ¹íˆ ìì—°ì–´ì²˜ë¦¬) ê³µìœ Â·ë°°í¬Â·ê´€ë¦¬ í”Œë«í¼**ì„ ë§Œë“œëŠ” íšŒì‚¬ì´ì\n",
        "**ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°**ì…ë‹ˆë‹¤.\n",
        "\n",
        "- íŠ¹íˆ â€˜Transformersâ€™ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìœ ëª…í•˜ë©°, ì—°êµ¬ìÂ·ê°œë°œìÂ·ê¸°ì—…ì´ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 2. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë€?\n",
        "**ìµœì‹  ë”¥ëŸ¬ë‹ ëª¨ë¸(Transformer ê¸°ë°˜)ì„ ì‰½ê²Œ ê°€ì ¸ë‹¤ ì“¸ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” íŒŒì´ì¬ ë„êµ¬**ì…ë‹ˆë‹¤.\n",
        "\n",
        "### âœ” ì£¼ìš” íŠ¹ì§•\n",
        "- ì‚¬ì „í•™ìŠµ(pretrained) ëª¨ë¸ ìˆ˜ì²œ ê°œë¥¼ 1~2ì¤„ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "- NLPÂ·ìŒì„±Â·ì‹œê°Â·ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›\n",
        "- PyTorch / TensorFlow / JAX ëª¨ë‘ ì§€ì›\n",
        "- í† í¬ë‚˜ì´ì €, ëª¨ë¸, íŒŒì´í”„ë¼ì¸ ë“± ê³ ìˆ˜ì¤€ API ì œê³µ\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 3. Transformersê°€ ì§€ì›í•˜ëŠ” ì£¼ìš” ì‘ì—…(Task)\n",
        "| ë¶„ë¥˜ | ì˜ˆì‹œ ì‘ì—… |\n",
        "|------|-----------|\n",
        "| **NLP** | í…ìŠ¤íŠ¸ ìƒì„±, ë²ˆì—­, ë¶„ë¥˜, ì§ˆì˜ì‘ë‹µ, ìš”ì•½, ê°ì • ë¶„ì„ |\n",
        "| **ëŒ€í™” ëª¨ë¸** | ChatGPT ìŠ¤íƒ€ì¼ ëŒ€í™” ìƒì„± |\n",
        "| **ì‹œê° ëª¨ë¸** | ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ íƒì§€ |\n",
        "| **ìŒì„± ëª¨ë¸** | ìŒì„± ì¸ì‹/ìŒì„± ìƒì„± |\n",
        "| **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸** | í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ëª¨ë¸(CLIP, BLIP ë“±) |\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 4. Transformersì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n",
        "\n",
        "### âœ” 1) Model (ëª¨ë¸)\n",
        "ì‚¬ì „í•™ìŠµëœ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ëŠ¥.\n",
        "\n",
        "```python\n",
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "```\n",
        "\n",
        "### âœ” 2) Tokenizer (í† í¬ë‚˜ì´ì €)\n",
        "ë¬¸ì¥ì„ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” **ìˆ«ì(token)** ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬.\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "```\n",
        "\n",
        "### âœ” 3) Pipeline (íŒŒì´í”„ë¼ì¸)\n",
        "ëª¨ë¸ + í† í¬ë‚˜ì´ì €ë¥¼ ë¬¶ì–´ì„œ íŠ¹ì • ì‘ì—…ì„ ì‰½ê²Œ ìˆ˜í–‰í•˜ëŠ” ê³ ìˆ˜ì¤€ API.\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I love this movie!\")\n",
        "```\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 5. Hugging Face Hub\n",
        "TransformersëŠ” â€˜Hubâ€™ë¼ëŠ” ëª¨ë¸ ì €ì¥ì†Œì™€ ì—°ë™ë©ë‹ˆë‹¤.\n",
        "\n",
        "### âœ” ê°€ëŠ¥í•œ ì¼\n",
        "- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
        "- ìì‹ ì˜ ëª¨ë¸ ì—…ë¡œë“œ\n",
        "- ë°ì´í„°ì…‹ ë° í‰ê°€ ì§€í‘œ ê³µìœ \n",
        "- ë‹¤ë¥¸ ì‚¬ëŒì˜ ëª¨ë¸ì„ ì¦‰ì‹œ ì‚¬ìš©\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 6. Transformersê°€ ì¸ê¸° ìˆëŠ” ì´ìœ \n",
        "| ì´ìœ  | ì„¤ëª… |\n",
        "|------|-------|\n",
        "| ì‰½ê²Œ ì‚¬ìš© | ëª¨ë¸Â·í† í¬ë‚˜ì´ì €ë¥¼ ë‹¨ í•œ ì¤„ë¡œ ë¡œë“œ |\n",
        "| ë‹¤ì–‘í•œ ëª¨ë¸ | ìˆ˜ì²œ ê°œì˜ pretrained ëª¨ë¸ ì§€ì› |\n",
        "| ì†ë„ | GPU/TPU ìµœì í™” |\n",
        "| í™˜ê²½ í†µí•© | PyTorchÂ·TFÂ·JAX ëª¨ë‘ í†µí•© ì§€ì› |\n",
        "| ì»¤ë®¤ë‹ˆí‹° | ë…¼ë¬¸ ëª¨ë¸ë“¤ì´ ê°€ì¥ ë¹¨ë¦¬ êµ¬í˜„Â·ê³µìœ  |\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 7. Transformers ì‚¬ìš© íë¦„ ìš”ì•½\n",
        "1ï¸âƒ£ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ  \n",
        "2ï¸âƒ£ ì…ë ¥ ë¬¸ì¥ í† í°í™”  \n",
        "3ï¸âƒ£ ëª¨ë¸ë¡œ ì¶”ë¡   \n",
        "4ï¸âƒ£ ì¶œë ¥ í† í°ì„ ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "inputs = tokenizer(\"Hello!\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "```\n",
        "\n",
        "---\n",
        "## ğŸ”¹ 8. Transformersë¡œ í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œ ì‹¤ìŠµ(í•™ìƒìš©)\n",
        "| ì‹¤ìŠµ | ë‚´ìš© |\n",
        "|-------|--------|\n",
        "| í…ìŠ¤íŠ¸ ìƒì„± | GPT ê³„ì—´ ëª¨ë¸ë¡œ ë¬¸ì¥ ì´ì–´ì“°ê¸° |\n",
        "| ë¬¸ì¥ ë¶„ë¥˜ | ê°ì • ë¶„ì„ ì‹¤ìŠµ |\n",
        "| ì§ˆë¬¸-ë‹µë³€ | ìì—°ì–´ ì§ˆì˜ì‘ë‹µ ëª¨ë¸ ì‹¤ìŠµ |\n",
        "| ë²ˆì—­ | ì˜ì–´ â†” í•œêµ­ì–´ ë²ˆì—­ |\n",
        "| ì±—ë´‡ | Instruct ëª¨ë¸ ê¸°ë°˜ ëŒ€í™” ìƒì„± |\n",
        "| ë¬¸ì„œ ìš”ì•½ | T5, BART ê¸°ë°˜ Summarization |\n",
        "\n",
        "---\n",
        "## ğŸ”¶ í•œ ë¬¸ì¥ ìš”ì•½\n",
        "**Hugging Face TransformersëŠ” ìµœì‹  AI ëª¨ë¸ì„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ”, NLPÂ·ì‹œê°Â·ìŒì„± ë¶„ì•¼ì˜ ê°€ì¥ ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.**\n"
      ],
      "id": "jLUrO3VKW3uY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}