{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adFzzFsB-Ofl"
      },
      "source": [
        "<h1>3ì¥ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ìì„¸íˆ ì‚´í´ ë³´ê¸°</h1>\n",
        "<i>ìƒì„± LLMì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ íƒí—˜í•˜ê¸°</i>\n",
        "\n",
        "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter03.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ <[í•¸ì¦ˆì˜¨ LLM](https://tensorflow.blog/handson-llm/)> ì±… 3ì¥ì˜ ì½”ë“œë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
        "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5dPfvShKZ4g"
      },
      "source": [
        "---\n",
        "\n",
        "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78YkTOy0LSXb",
        "outputId": "b09dacaf-ab6e-47c3-ab25-23f18df00125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.10.5)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ],
      "source": [
        "# Phi-3 ëª¨ë¸ê³¼ í˜¸í™˜ì„± ë•Œë¬¸ì— transformers 4.48.3 ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "8lVE_3idzVyF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_23Z_do-faF"
      },
      "source": [
        "# LLM ë¡œë“œí•˜ê¸°\n",
        "\n",
        "| í•­ëª©                     | ì„¤ëª…                                     |\n",
        "| ---------------------- | -------------------------------------- |\n",
        "| `AutoTokenizer`        | ë¬¸ì¥ â†” í† í° ë³€í™˜ ë‹´ë‹¹                          |\n",
        "| `AutoModelForCausalLM` | í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸(Phi-3) ë¡œë“œ                    |\n",
        "| GPU ì‚¬ìš©                 | `device_map=\"cuda\"`                    |\n",
        "| dtype ìë™ ì„¤ì •            | `torch_dtype=\"auto\"`                   |\n",
        "| pipeline ëª©ì             | ëª¨ë¸ + í† í¬ë‚˜ì´ì €ë¥¼ ì†ì‰½ê²Œ ì‚¬ìš©í•˜ë„ë¡ ë¬¶ì–´ì¤Œ              |\n",
        "| ìƒì„± ì˜µì…˜                  | `max_new_tokens=50`, `do_sample=False` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5RLd6dI-Ytm",
        "outputId": "928ea91b-7d48-4fbc-c695-b26ae2561e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "#    - \"microsoft/Phi-3-mini-4k-instruct\" ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ”\n",
        "#      í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë©”ëª¨ë¦¬ì— ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "#    - í† í¬ë‚˜ì´ì €ëŠ” ë¬¸ì¥ì„ 'í† í°(token)'ì´ë¼ëŠ” ìˆ«ì ëª©ë¡ìœ¼ë¡œ ë°”ê¾¸ê³ ,\n",
        "#      ë‹¤ì‹œ ìˆ«ìë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³µì›í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "# -----------------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. ì‚¬ì „í•™ìŠµ ì–¸ì–´ ëª¨ë¸ ë¡œë“œ\n",
        "#    - AutoModelForCausalLM : â€œë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸â€ì„ ë¶ˆëŸ¬ì˜¬ ë•Œ ì‚¬ìš©\n",
        "#    - device_map=\"cuda\"     : GPUë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ë¹ ë¥´ê²Œ ì²˜ë¦¬\n",
        "#    - torch_dtype=\"auto\"    : ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ìµœì  ë°ì´í„° íƒ€ì… ì„ íƒ\n",
        "#    - trust_remote_code=True:\n",
        "#         ì¼ë¶€ ëª¨ë¸ì€ ìì²´ ì»¤ìŠ¤í…€ ì½”ë“œê°€ í•„ìš”í•˜ë¯€ë¡œ,\n",
        "#         Hugging Face Hubì— ìˆëŠ” ëª¨ë¸ ì½”ë“œë¥¼ ì‹ ë¢°í•˜ê³  ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •\n",
        "# -----------------------------------------------------------\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "#    - pipeline(\"text-generation\")\n",
        "#         â†’ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¬¶ì–´ì¤˜ì„œ\n",
        "#           â€œë¬¸ì¥ì„ ì´ì–´ì„œ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥â€ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
        "#\n",
        "#    ì£¼ìš” ì˜µì…˜ ì„¤ëª…:\n",
        "#    - return_full_text=False :\n",
        "#         ì…ë ¥(prompt) ë¬¸ì¥ì„ ì¶œë ¥ì—ì„œ ì œì™¸í•˜ê³ \n",
        "#         ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ë°˜í™˜\n",
        "#\n",
        "#    - max_new_tokens=50 :\n",
        "#         ìƒˆë¡œ ë§Œë“¤ì–´ë‚¼ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´(í† í° ìˆ˜)\n",
        "#\n",
        "#    - do_sample=False :\n",
        "#         ëœë¤ ì„ íƒ ì—†ì´ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë§Œ ê³ ë¥´ëŠ” ë°©ì‹\n",
        "#         â†’ ì•ˆì •ì , ë°˜ë³µ ê°€ëŠ¥í•œ ê²°ê³¼ ìƒì„±\n",
        "# -----------------------------------------------------------\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REqcz-ID_XgV"
      },
      "source": [
        "# í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ LLMì˜ ì…ë ¥ê³¼ ì¶œë ¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17h6TPHluJ-i",
        "outputId": "1897f876-7d11-4985-9cdb-ce7a6b89a084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------\n",
        "# 1. ëª¨ë¸ì—ê²Œ ë³´ë‚¼ ìš”ì²­(prompt) ì‘ì„±\n",
        "#    - ì‚¬ìš©ìê°€ ëª¨ë¸ì—ê²Œ â€œë¬´ì—‡ì„ ìƒì„±í•´ ë‹¬ë¼â€ê³  ì§€ì‹œí•˜ëŠ” ë¬¸ì¥ì…ë‹ˆë‹¤.\n",
        "#    - ì—¬ê¸°ì„œëŠ” â€œì •ì›ì—ì„œ ì¼ì–´ë‚œ ì‚¬ê³ ì— ëŒ€í•´ ì‚¬ë¼ì—ê²Œ ì‚¬ê³¼ ì´ë©”ì¼ ì‘ì„±â€\n",
        "#      ìš”ì²­ì„ ëª¨ë¸ì—ê²Œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "# -----------------------------------------------------------\n",
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. ìƒì„± íŒŒì´í”„ë¼ì¸(generator)ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ìƒì„±\n",
        "#    - generator(prompt) : í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ìœ¼ë©´ ëª¨ë¸ì´ ë¬¸ì¥ì„ ì´ì–´ì„œ ìƒì„±í•´ ì¤ë‹ˆë‹¤.\n",
        "#    - outputì€ ì—¬ëŸ¬ ê°œì˜ ê²°ê³¼ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
        "# -----------------------------------------------------------\n",
        "output = generator(prompt)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. ìƒì„±ëœ ë¬¸ì¥ ì¶œë ¥\n",
        "#    - output[0] : ì²« ë²ˆì§¸ ìƒì„± ê²°ê³¼\n",
        "#    - ['generated_text'] : ëª¨ë¸ì´ ìƒˆë¡œ ë§Œë“¤ì–´ë‚¸ í…ìŠ¤íŠ¸ ë¶€ë¶„\n",
        "# -----------------------------------------------------------\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| í•­ëª©    | ì„¤ëª…                                            |\n",
        "| ----- | --------------------------------------------- |\n",
        "| ì¶œë ¥ ë‚´ìš© | Phi-3 ëª¨ë¸ì˜ ì „ì²´ êµ¬ì¡°(ëª¨ë“ˆ, ë¸”ë¡, ë ˆì´ì–´)                  |\n",
        "| í¬í•¨ ìš”ì†Œ | ì„ë² ë”©, Transformer Layer, LayerNorm, lm_head    |\n",
        "| ëª©ì     | ëª¨ë¸ì´ ì–´ë–¤ ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸                      |\n",
        "| ìœ ìš©ì„±   | ë””ë²„ê¹…, ëª¨ë¸ ì´í•´, êµ¬ì¡° ë¹„êµ ë“±ì— í™œìš©                       |\n",
        "| íŠ¹ì§•    | GPT ê³„ì—´ì²˜ëŸ¼ **Decoder-only Transformer** êµ¬ì¡°ë¡œ í‘œì‹œë¨ |\n"
      ],
      "metadata": {
        "id": "1JhFeyqrpDOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoFkdTd6_g5o",
        "outputId": "736ef8d5-2d61-474e-970f-18993f07f851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "          (rotary_emb): Phi3RotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm()\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_attention_layernorm): Phi3RMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTrwzB67BYVY"
      },
      "source": [
        "# í™•ë¥  ë¶„í¬ë¡œë¶€í„° í•˜ë‚˜ì˜ í† í° ì„ íƒí•˜ê¸°(ìƒ˜í”Œë§/ë””ì½”ë”©)\n",
        "\n",
        "| í•­ëª©                | ì„¤ëª…                                                         |\n",
        "| ----------------- | ---------------------------------------------------------- |\n",
        "| `input_ids`       | í”„ë¡¬í”„íŠ¸(\"The capital of France is\")ë¥¼ ìˆ«ì í† í°ìœ¼ë¡œ ë³€í™˜í•œ ê°’            |\n",
        "| `.to(\"cuda\")`     | ì…ë ¥ì„ GPUì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ì´ë™                                         |\n",
        "| `model.model()`   | Transformer ë³¸ì²´(Attention + FFN)ë§Œ ì‹¤í–‰í•˜ì—¬ **hidden states** ìƒì„± |\n",
        "| `model_output[0]` | ê° í† í°ì˜ ì€ë‹‰ ìƒíƒœ(hidden vector)                                 |\n",
        "| `model.lm_head()` | hidden stateë¥¼ vocabulary í¬ê¸°ì˜ **logits(ë‹¨ì–´ ì ìˆ˜)** ë¡œ ë³€í™˜        |\n",
        "| ì—­í•  ë¶„ë¦¬             | `model.model` = ë³€í™˜ê¸° / `lm_head` = ë‹¨ì–´ ì˜ˆì¸¡ê¸°                   |\n",
        "| ì „ì²´ ëª©ì              | â€œë‹¤ìŒ í† í°ì´ ë  ë‹¨ì–´ ë¶„í¬â€ë¥¼ ë‚´ë¶€ ë‹¨ê³„ë³„ë¡œ ì§ì ‘ í™•ì¸                            |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sEcxYgJxBYbJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”\n",
        "#    - tokenizer()ëŠ” ë¬¸ì¥ì„ ìˆ«ì í† í°(token)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "#    - return_tensors=\"pt\" : PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜\n",
        "# ---------------------------------------------------------\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ëª¨ë¸ì´ GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì…ë ¥ì„ GPUë¡œ ì´ë™\n",
        "# ---------------------------------------------------------\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Transformer ë³¸ì²´(model.model)ë§Œ ì‹¤í–‰\n",
        "#    - model.model :\n",
        "#         Phi-3ì˜ \"Transformer ë¸”ë¡\" ë¶€ë¶„\n",
        "#         (ì„ë² ë”©, Attention, FFN ë“±)\n",
        "#\n",
        "#    - ì´ ë‹¨ê³„ì—ì„œëŠ” ë‹¨ìˆœíˆ â€œë‹¤ìŒ í† í° ì˜ˆì¸¡ ì „ì— í•„ìš”í•œ ë³€í™˜â€\n",
        "#      (ì¦‰, ì€ë‹‰ ìƒíƒœ hidden states)ë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "# ---------------------------------------------------------\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. lm_headì„ í†µí•´ ì‹¤ì œ ë‹¨ì–´ ì˜ˆì¸¡ ì ìˆ˜(logits) ê³„ì‚°\n",
        "#    - model.lm_head :\n",
        "#         Transformerê°€ ê³„ì‚°í•œ hidden statesë¥¼\n",
        "#         ì‹¤ì œ \"ë‹¨ì–´ í™•ë¥  ë²¡í„°\"ë¡œ ë³€í™˜í•˜ëŠ” ë§ˆì§€ë§‰ ì¸µ\n",
        "#\n",
        "#    - lm_head_output :\n",
        "#         ê° í† í°ë§ˆë‹¤ vocabulary ì „ì²´ì— ëŒ€í•œ ì ìˆ˜(logits)\n",
        "# ---------------------------------------------------------\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| ì½”ë“œ                           | ì˜ë¯¸                           |\n",
        "| ---------------------------- | ---------------------------- |\n",
        "| `lm_head_output[0, -1]`      | ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ ë‹¨ì–´ ì ìˆ˜(logits)     |\n",
        "| `.argmax(-1)`                | ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ì˜ token ID ì„ íƒ |\n",
        "| `tokenizer.decode(token_id)` | ìˆ«ì ID â†’ ì‹¤ì œ ë‹¨ì–´ë¡œ ë³µì›            |\n",
        "| ì „ì²´ ëª©ì                         | ëª¨ë¸ ë‚´ë¶€ì—ì„œ **ë‹¤ìŒ ë‹¨ì–´ ì§ì ‘ ì˜ˆì¸¡**í•˜ê¸°    |\n",
        "| ì˜ˆì‹œ                           | `\"Paris\"`                    |\n"
      ],
      "metadata": {
        "id": "T9RbDPoNpizF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "68YUSS4GBf9Q",
        "outputId": "7ce76ceb-3f67-4d66-a9c1-043636d1491c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1 â†’ batch size (ì…ë ¥ ë¬¸ì¥ ìˆ˜)\n",
        "\n",
        "- sequence_length â†’ í”„ë¡¬í”„íŠ¸ê°€ í† í°í™”ëœ í† í° ê°œìˆ˜\n",
        "  - ì˜ˆ: \"The capital of France is\" â†’ 5~9 tokens ì •ë„\n",
        "\n",
        "- hidden_size â†’ Phi-3ì˜ ì€ë‹‰ ë²¡í„° ì°¨ì›\n",
        "  - (Phi-3 mini ëª¨ë¸ì€ ì•½ 3072ì°¨ì› ì‚¬ìš©)"
      ],
      "metadata": {
        "id": "tnNXiamJp5qr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWWrfC5oBjwp",
        "outputId": "167e5460-63ef-49fc-dcbf-7bb33b364085"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model_output[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í”„ë¡¬í”„íŠ¸ì˜ ê° í† í°ì— ëŒ€í•´ ë‹¤ìŒ í† í°ì„ ë¬´ì—‡ìœ¼ë¡œ ì˜ˆì¸¡í• ì§€ ì ìˆ˜ë¥¼ ë‹´ì€ ë°ì´í„°\n",
        "\n",
        "- ì¦‰,\n",
        "ì…ë ¥ëœ ë¬¸ì¥ì˜ ê° ìœ„ì¹˜ì—ì„œ â€˜ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ í›„ë³´â€™ ì „ì²´ì— ëŒ€í•œ ì ìˆ˜í‘œ"
      ],
      "metadata": {
        "id": "Y5rwNSxoqMkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC1PdOnTBnxZ",
        "outputId": "379f59ae-d5c7-4a51-a4c7-fc7f955fcb9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "lm_head_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2_rP4QBqrZ"
      },
      "source": [
        "# í‚¤ì™€ ê°’ì„ ìºì‹±í•˜ì—¬ ìƒì„± ì†ë„ ë†’ì´ê¸°\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMì—ì„œì˜ ìºì‹œ(KV Cache)ë€?\n",
        "\n",
        "- LLMì´ ë¬¸ì¥ì„ ìƒì„±í•  ë•Œ, Transformerì˜ Self-Attentionì€\n",
        "â€œì§€ê¸ˆ ìƒì„±í•˜ëŠ” í† í°ì´ ì•ì˜ ëª¨ë“  í† í°ì„ ì°¸ì¡°â€í•´ì•¼ í•œë‹¤.\n",
        "\n",
        "- ì¦‰,\n",
        "- 1ë²ˆì§¸ í† í° ìƒì„± â†’ ì…ë ¥ ì „ì²´ ì‚¬ìš©\n",
        "- 2ë²ˆì§¸ í† í° ìƒì„± â†’ 1ë²ˆì§¸ ê²°ê³¼ê¹Œì§€ í¬í•¨\n",
        "- 3ë²ˆì§¸ í† í° ìƒì„± â†’ 1, 2ë²ˆì§¸ ê²°ê³¼ í¬í•¨\n",
        "- â€¦\n",
        "- 100ë²ˆì§¸ í† í° ìƒì„± â†’ ì•ì˜ 99ê°œ í† í°ê³¼ attention ê³„ì‚°\n",
        "\n",
        "- ì´ë ‡ê²Œ ë˜ë©´ ë§¤ í† í°ë§ˆë‹¤ ëª¨ë“  ê³¼ê±° í† í°ê³¼ì˜ ê³„ì‚°ì„ ë°˜ë³µí•´ì•¼ í•˜ë¯€ë¡œ\n",
        "ì†ë„ê°€ í¬ê²Œ ëŠë ¤ì§„ë‹¤."
      ],
      "metadata": {
        "id": "qzUxcci6rpS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê·¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê¸°ìˆ ì´ ë°”ë¡œ KV Cache (Key/Value Cache)\n",
        "\n",
        "- Transformerì˜ Self-Attentionì—ì„œ ì‚¬ìš©ë˜ëŠ” ìš”ì†Œ:\n",
        "\n",
        "  - Query (Q)\n",
        "\n",
        "  - Key (K)\n",
        "\n",
        "  - Value (V)\n",
        "\n",
        "- ë¬¸ì¥ì„ ìƒì„±í•  ë•Œ Kì™€ VëŠ” ì´ì „ í† í°ì—ì„œ ì´ë¯¸ ê³„ì‚°í•œ ê°’ë“¤ì´ë‹¤.\n",
        "\n",
        "- ê·¸ë˜ì„œ, â€œë§¤ë²ˆ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ë§ê³ , ì´ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°í•œ K/V ê°’ì„ ì €ì¥í•´ë‘ì!â€\n",
        "\n",
        "- ì´ê²ƒì´ ë°”ë¡œ KV ìºì‹±ì´ë‹¤."
      ],
      "metadata": {
        "id": "WIK0a4Cgr7uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| ì„¤ì •                | íŠ¹ì§•                           | ì†ë„       |\n",
        "| ----------------- | ---------------------------- | -------- |\n",
        "| `use_cache=True`  | ìºì‹œ í™œìš© â†’ ê³¼ê±° ê³„ì‚° ì¬ì‚¬ìš©            | ë¹ ë¦„ âš¡     |\n",
        "| `use_cache=False` | ë§¤ í† í°ë§ˆë‹¤ ëª¨ë“  attention ê³„ì‚° ë‹¤ì‹œ ìˆ˜í–‰ | ë§¤ìš° ëŠë¦¼ ğŸ¢ |\n"
      ],
      "metadata": {
        "id": "tLVG79YisOao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B0n6JhNHBrin"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. í”„ë¡¬í”„íŠ¸(ì…ë ¥ ë¬¸ì¥)ë¥¼ í† í°í™”\n",
        "#    - tokenizer()ëŠ” ë¬¸ì¥ì„ ìˆ«ì í† í° ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "#    - return_tensors='pt' : PyTorch í…ì„œ í˜•íƒœë¡œ ë³€í™˜\n",
        "# ---------------------------------------------------------\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ì…ë ¥ í† í°ì„ GPUë¡œ ì´ë™\n",
        "#    - ëª¨ë¸ì´ GPUì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì…ë ¥ë„ GPUë¡œ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# ---------------------------------------------------------\n",
        "input_ids = input_ids.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| í•­ëª©                   | ì„¤ëª…                           |\n",
        "| -------------------- | ---------------------------- |\n",
        "| `%%timeit -n 1`      | ì…€ ì‹¤í–‰ ì‹œê°„ì„ í•œ ë²ˆ ì¸¡ì •              |\n",
        "| `model.generate()`   | Phi-3 ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜       |\n",
        "| `input_ids`          | í”„ë¡¬í”„íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•œ ê°’             |\n",
        "| `max_new_tokens=100` | ìƒˆë¡œ ìƒì„±í•  ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´             |\n",
        "| `use_cache=True`     | Transformer ìºì‹œ ì‚¬ìš© â†’ ìƒì„± ì†ë„ ì¦ê°€ |\n",
        "| ê²°ê³¼                   | í…ìŠ¤íŠ¸ ìƒì„±ì— ê±¸ë¦¬ëŠ” ì‹œê°„ + ìƒì„±ëœ í† í° ê²°ê³¼   |\n"
      ],
      "metadata": {
        "id": "Okcs1vn_qrti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwIvt6jSByAF",
        "outputId": "d8e1fece-7703-4523-fbe5-fafd492d0c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.11 s Â± 289 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "# ---------------------------------------------------------\n",
        "# 1. %%timeit -n 1\n",
        "#    - ì´ ì…€ì„ ì‹¤í–‰í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” Jupyter ë§¤ì§ ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
        "#    - \"-n 1\"ì€ \"í•œ ë²ˆë§Œ ì‹¤í–‰í•´ì„œ ì‹œê°„ì„ ì¬ë¼\"ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
        "#    - ì¦‰, ì´ generate() í•¨ìˆ˜ê°€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ê±¸ë¦¬ëŠ”ì§€\n",
        "#      ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. í…ìŠ¤íŠ¸ ìƒì„±\n",
        "#    - model.generate()ëŠ” LLMì´ í”„ë¡¬í”„íŠ¸(input_ids)ì— ì´ì–´ì„œ\n",
        "#      ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "#\n",
        "#    ì£¼ìš” ì˜µì…˜ ì„¤ëª…:\n",
        "#    - max_new_tokens=100 :\n",
        "#         ìƒˆë¡œ ìƒì„±ë  í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´(í† í° ìˆ˜)\n",
        "#\n",
        "#    - use_cache=True  :\n",
        "#         Transformerì˜ ìºì‹œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ ë” ë¹ ë¥´ê²Œ ìƒì„±\n",
        "#         (ì´ì „ ìƒì„± ê³¼ì •ì˜ key/valueë¥¼ ì¬ì‚¬ìš©)\n",
        "# ---------------------------------------------------------\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=100,\n",
        "    use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| í•­ëª©                   | ì„¤ëª…                                        |\n",
        "| -------------------- | ----------------------------------------- |\n",
        "| `%%timeit -n 1`      | ì…€ ì‹¤í–‰ ì‹œê°„ì„ í•œ ë²ˆ ì¸¡ì •                           |\n",
        "| `max_new_tokens=100` | 100ê°œì˜ ìƒˆ í† í° ìƒì„±                             |\n",
        "| `use_cache=False`    | ìºì‹œ ë¹„í™œì„±í™” â†’ ë§¤ ë‹¨ê³„ ì „ì²´ ê³„ì‚° ë°˜ë³µ â†’ **ì†ë„ ë§¤ìš° ëŠë¦¼**    |\n",
        "| ìºì‹œë€?                 | ì´ì „ ìƒì„± ë‹¨ê³„ì˜ í‚¤/ê°’(attention ê³„ì‚° ê²°ê³¼)ì„ ì €ì¥í•´ ë‘ëŠ” ê¸°ëŠ¥ |\n",
        "| ê²°ê³¼                   | LLM ìƒì„± ì†ë„ ì‹¤í—˜(ìºì‹œ ON/OFF ë¹„êµ ëª©ì )             |\n"
      ],
      "metadata": {
        "id": "M00fHVVArJCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFb1dcvJByCW",
        "outputId": "fb3152fe-bfe9-4ebf-88c2-8e3fb9fb3eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32.3 s Â± 306 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "# ---------------------------------------------------------\n",
        "# 1. %%timeit -n 1\n",
        "#    - ì´ Jupyter ë§¤ì§ ëª…ë ¹ì–´ëŠ” ì…€ ì „ì²´ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
        "#    - \"-n 1\"ì€ \"1ë²ˆë§Œ ì‹¤í–‰í•´ì„œ ì‹œê°„ ì¸¡ì •\"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "#    - ì¦‰, í…ìŠ¤íŠ¸ ìƒì„± ì†ë„ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. í…ìŠ¤íŠ¸ ìƒì„±\n",
        "#    - model.generate()ëŠ” LLMì´ í”„ë¡¬í”„íŠ¸(input_ids)ì— ì´ì–´ì„œ\n",
        "#      í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "#\n",
        "#    - ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì˜µì…˜:\n",
        "#\n",
        "#      use_cache=False\n",
        "#        â†’ Transformerì˜ â€œKV ìºì‹œ(Key/Value cache)â€ ê¸°ëŠ¥ì„ ë•ë‹ˆë‹¤.\n",
        "#        â†’ ìºì‹œë¥¼ ì“°ì§€ ì•Šìœ¼ë©´, ë§¤ í† í° ìƒì„± ì‹œ\n",
        "#           ì´ì „ ëª¨ë“  tokenì— ëŒ€í•œ ì—°ì‚°ì„ ë‹¤ì‹œ ìˆ˜í–‰í•´ì•¼ í•˜ë¯€ë¡œ\n",
        "#           í›¨ì”¬ ëŠë ¤ì§‘ë‹ˆë‹¤.\n",
        "#\n",
        "#      max_new_tokens=100\n",
        "#        â†’ ìƒˆë¡œ ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´\n",
        "# ---------------------------------------------------------\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=100,\n",
        "    use_cache=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}